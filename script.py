import logging
import asyncio
import sys
import aiohttp
from bs4 import BeautifulSoup
from aiogram import Bot, Dispatcher, types, F
from aiogram.client.bot import DefaultBotProperties
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton, ReplyKeyboardRemove, CallbackQuery
from aiogram.filters import Command
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.context import FSMContext
import json
import os
import subprocess
import html
from datetime import datetime
import urllib.parse
import kss
import networkx as nx
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

MODEL_NAME = "EbanLee/kobart-summary-v3"
tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)
model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logfile.log"),
        logging.StreamHandler()
    ]
)

# --- ìƒìˆ˜ ë° í™˜ê²½ ë³€ìˆ˜ ---
URL = 'https://www.pknu.ac.kr/main/163'
BASE_URL = 'https://www.pknu.ac.kr'
CATEGORY_CODES = {
    "ì „ì²´": "",
    "ê³µì§€ì‚¬í•­": "10001",
    "ë¹„êµê³¼ ì•ˆë‚´": "10002",
    "í•™ì‚¬ ì•ˆë‚´": "10003",
    "ë“±ë¡/ì¥í•™": "10004",
    "ì´ˆë¹™/ì±„ìš©": "10007"
}
TOKEN = os.environ.get('TELEGRAM_TOKEN')
CHAT_ID = os.environ.get('CHAT_ID')

# --- ë´‡ ë° Dispatcher ì´ˆê¸°í™” ---
bot = Bot(token=TOKEN, default=DefaultBotProperties(parse_mode="HTML"))
dp = Dispatcher(bot=bot)

# --- FSM ìƒíƒœ ì •ì˜ ---
class FilterState(StatesGroup):
    waiting_for_date = State()
    selecting_category = State()

# --- ë‚ ì§œ íŒŒì‹± í•¨ìˆ˜ ---
def parse_date(date_str):
    try:
        return datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError as ve:
        logging.error(f"Date parsing error for {date_str}: {ve}")
        return None

# --- HTTP ìš”ì²­ í•¨ìˆ˜ (fetch_url) ---
async def fetch_url(url):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as response:
                if response.status != 200:
                    logging.error(f"âŒ HTTP ìš”ì²­ ì‹¤íŒ¨ ({response.status}): {url}")
                    return None
                return await response.text()
    except Exception as e:
        logging.error(f"âŒ URL ìš”ì²­ ì˜¤ë¥˜: {url}, {e}")
        return None

# --- ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ---
async def get_school_notices(category=""):
    try:
        category_url = f"{URL}?cd={category}" if category else URL
        html_content = await fetch_url(category_url)
        soup = BeautifulSoup(html_content, 'html.parser')
        notices = []
        for tr in soup.find_all("tr"):
            title_td = tr.find("td", class_="bdlTitle")
            user_td = tr.find("td", class_="bdlUser")
            date_td = tr.find("td", class_="bdlDate")
            if title_td and title_td.find("a") and user_td and date_td:
                a_tag = title_td.find("a")
                title = a_tag.get_text(strip=True)
                href = a_tag.get("href")
                if href.startswith("/"):
                    href = BASE_URL + href
                elif href.startswith("?"):
                    href = BASE_URL + "/main/163" + href
                elif not href.startswith("http"):
                    href = BASE_URL + "/" + href
                department = user_td.get_text(strip=True)
                date = date_td.get_text(strip=True)
                notices.append((title, href, department, date))
        notices.sort(key=lambda x: parse_date(x[3]) or datetime.min, reverse=True)
        return notices
    except Exception as e:
        logging.exception("âŒ Error in get_school_notices")
        return []

# --- TextRank ê¸°ë°˜ ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ ---
def text_rank_key_sentences(text, top_n=5):
    """
    TextRank ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ ì¤‘ìš” ë¬¸ì¥ ì„ ë³„
    """
    sentences = kss.split_sentences(text)
    if len(sentences) <= top_n:
        return sentences  # ë¬¸ì¥ì´ ì ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    # TF-IDF ë²¡í„°í™”
    vectorizer = TfidfVectorizer()
    sentence_vectors = vectorizer.fit_transform(sentences).toarray()

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarity_matrix = cosine_similarity(sentence_vectors, sentence_vectors)

    # ê·¸ë˜í”„ ìƒì„± ë° PageRank ì ìš©
    nx_graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(nx_graph)

    # ì¤‘ìš”ë„ê°€ ë†’ì€ ë¬¸ì¥ ì •ë ¬ í›„ ì„ íƒ
    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    key_sentences = [s for _, s in ranked_sentences[:top_n]]

    return key_sentences

def clean_and_format_text(text):
    """
    - ì¤‘ë³µ ë‹¨ì–´ ë° ë°˜ë³µëœ í‘œí˜„ ì œê±°
    - ë¬¸ì¥ ë§ˆì¹¨í‘œ ì¶”ê°€
    - ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
    """
    if not text.strip():
        return text  # ë¹ˆ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    # 1ï¸âƒ£ ì¤‘ë³µ ë‹¨ì–´ ë° ë°˜ë³µëœ í‘œí˜„ ì œê±° (ì •ê·œì‹ ëŒ€ì‹  OrderedDict í™œìš©)
    words = text.split()
    cleaned_words = list(OrderedDict.fromkeys(words))  # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
    text = " ".join(cleaned_words)

    # 2ï¸âƒ£ ë¬¸ì¥ ë§ˆì¹¨í‘œ ë³´ì •
    sentences = kss.split_sentences(text)  # ë¬¸ì¥ ë¶„ë¦¬
    cleaned_sentences = []
    for sentence in sentences:
        if not sentence.endswith(('.', '!', '?', '"', "'")):
            sentence += "."  # ë¬¸ì¥ ëì´ ì´ìƒí•˜ë©´ ë§ˆì¹¨í‘œ ì¶”ê°€
        cleaned_sentences.append(sentence)

    # 3ï¸âƒ£ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì ìš© (ê°€ë…ì„± í–¥ìƒ)
    formatted_text = "\n".join(cleaned_sentences).strip()  # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°

    return formatted_text

def extract_key_sentences(text, top_n=5):
    """
    ì¤‘ìš” ë¬¸ì¥ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
    TextRank ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ìƒìœ„ Nê°œì˜ ë¬¸ì¥ì„ ì„ íƒ.
    """
    sentences = kss.split_sentences(text)
    
    # ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” ë‹¨ì–´ë¥¼ ì„ ë³„
    word_count = Counter(" ".join(sentences).split())
    important_words = [word for word, count in word_count.most_common(20)]  # ìƒìœ„ 20ê°œ ë‹¨ì–´ ì„ íƒ
    
    # ì¤‘ìš” ë‹¨ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ í•„í„°ë§
    key_sentences = []
    for sentence in sentences:
        if any(word in sentence for word in important_words):
            key_sentences.append(sentence)

    return key_sentences[:top_n]  # ìƒìœ„ Nê°œ ë¬¸ì¥ ì„ íƒ

# --- í…ìŠ¤íŠ¸ ìš”ì•½ ---
def summarize_text(text):
    """
    1. TextRankë¡œ ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ
    2. KoBART ëª¨ë¸ë¡œ ìš”ì•½
    """
    key_sentences = text_rank_key_sentences(text, top_n=7)
    combined_text = " ".join(key_sentences)

    inputs = tokenizer(combined_text, return_tensors="pt", padding=True, truncation=True, max_length=1024)
    try:
    summary_ids = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        num_beams=6,
        length_penalty=1.0,
        max_length=100,
        min_length=30,
        repetition_penalty=1.5,
        no_repeat_ngram_size=15,
    )
    except Exception as e:
        logging.error(f"âŒ KoBART ìš”ì•½ ì˜¤ë¥˜: {e}")
        return "ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."


    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# --- ì½˜í…ì¸  ì¶”ì¶œ: bdvTxt_wrap ì˜ì—­ ë‚´ í…ìŠ¤íŠ¸ì™€ /upload/ ì´ë¯¸ì§€ í¬ë¡¤ë§ ---
async def extract_content(url):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as response:
                html_content = await response.text()
        soup = BeautifulSoup(html_content, 'html.parser')
        # íŠ¹ì • ì˜ì—­ ë‚´ì˜ ì½˜í…ì¸  ì¶”ì¶œ
        container = soup.find("div", class_="bdvTxt_wrap")
        if not container:
            container = soup
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ: í•´ë‹¹ ì˜ì—­ì˜ <p> íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜´
        paragraphs = container.find_all('p')
        raw_text = ' '.join([para.get_text(separator=" ", strip=True) for para in paragraphs])
        # ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½ í›„ ê°€ë…ì„± ì •ë¦¬
        summary_text = clean_and_format_text(summarize_text(raw_text))
        # ì´ë¯¸ì§€ ì¶”ì¶œ: /upload/ ê²½ë¡œê°€ í¬í•¨ëœ URLë§Œ ì„ íƒ
        images = container.find_all('img')
        image_urls = []
        for img in images:
            src = img.get('src')
            if src:
                if not src.startswith(('http://', 'https://')):
                    src = urllib.parse.urljoin(url, src)
                if "/upload/" not in src:
                    continue
                if await is_valid_url(src):
                    image_urls.append(src)
        return summary_text, image_urls
    except Exception as e:
        logging.error(f"âŒ Failed to fetch content from {url}: {e}")
        return "", []

async def is_valid_url(url):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.head(url, timeout=10) as response:
                return response.status == 200
    except Exception as e:
        logging.error(f"âŒ Invalid image URL: {url}, error: {e}")
    return False

# --- JSON íŒŒì¼ ì²˜ë¦¬ (ê³µì§€ì‚¬í•­ ì¤‘ë³µ ì²´í¬) ---
def load_seen_announcements():
    try:
        with open("announcements_seen.json", "r", encoding="utf-8") as f:
            seen_data = json.load(f)
            return {(item[0], item[1]) if len(item) == 2 else tuple(item) for item in seen_data}
    except (FileNotFoundError, json.JSONDecodeError):
        return set()

def save_seen_announcements(seen):
    try:
        with open("announcements_seen.json", "w", encoding="utf-8") as f:
            json.dump([list(item) for item in seen], f, ensure_ascii=False, indent=4)
        push_changes()
    except Exception as e:
        logging.error(f"âŒ Failed to save announcements_seen.json and push to GitHub: {e}")

def push_changes():
    try:
        pat = os.environ.get("MY_PAT")
        if not pat:
            logging.error("âŒ GitHub PATê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Pushë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
            return
        os.environ["GIT_ASKPASS"] = "echo"
        os.environ["GIT_PASSWORD"] = pat
        subprocess.run(["git", "config", "--global", "credential.helper", "store"], check=True)
        subprocess.run(["git", "add", "announcements_seen.json"], check=True)
        subprocess.run(["git", "commit", "-m", "Update announcements_seen.json"], check=True)
        subprocess.run(["git", "push", "origin", "main"], check=True)
        logging.info("âœ… Successfully pushed changes to GitHub.")
    except subprocess.CalledProcessError as e:
        logging.error(f"âŒ ERROR: Failed to push changes to GitHub: {e}")

async def check_for_new_notices():
    logging.info("Checking for new notices...")
    seen_announcements = load_seen_announcements()
    logging.info(f"Loaded seen announcements: {seen_announcements}")
    current_notices = await get_school_notices()
    logging.info(f"Fetched current notices: {current_notices}")
    seen_titles_urls = {(title, url) for title, url, *_ in seen_announcements}
    new_notices = [
        (title, href, department, date) for title, href, department, date in current_notices
        if (title, href) not in seen_titles_urls
    ]
    logging.info(f"DEBUG: New notices detected: {new_notices}")
    if new_notices:
        for notice in new_notices:
            await send_notification(notice)
        seen_announcements.update(new_notices)
        save_seen_announcements(seen_announcements)
        logging.info(f"DEBUG: Updated seen announcements (after update): {seen_announcements}")
    else:
        logging.info("âœ… ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

@dp.message(Command("checknotices"))
async def manual_check_notices(message: types.Message):
    new_notices = await check_for_new_notices()
    if new_notices:
        await message.answer(f"ğŸ“¢ {len(new_notices)}ê°œì˜ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤!")
    else:
        await message.answer("âœ… ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

async def send_notification(notice):
    title, href, department, date = notice
    summary_text, image_urls = await extract_content(href)
    message_text = f"[ë¶€ê²½ëŒ€ <b>{html.escape(department)}</b> ê³µì§€ì‚¬í•­ ì—…ë°ì´íŠ¸]\n\n"
    message_text += f"<b>{html.escape(title)}</b>\n\n{html.escape(date)}\n\n"
    message_text += f"{html.escape(summary_text)}"
    if image_urls:
        message_text += "\n\n[ì²¨ë¶€ ì´ë¯¸ì§€]\n" + "\n".join(image_urls)
    keyboard = InlineKeyboardMarkup(inline_keyboard=[[InlineKeyboardButton(text="ìì„¸íˆ ë³´ê¸°", url=href)]])
    await bot.send_message(chat_id=CHAT_ID, text=message_text, reply_markup=keyboard)

@dp.message(Command("start"))
async def start_command(message: types.Message):
    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="ğŸ“…ë‚ ì§œ ì…ë ¥", callback_data="filter_date"),
         InlineKeyboardButton(text="ğŸ“¢ì „ì²´ ê³µì§€ì‚¬í•­", callback_data="all_notices")]
    ])
    await message.answer("ì•ˆë…•í•˜ì„¸ìš”! ê³µì§€ì‚¬í•­ ë´‡ì…ë‹ˆë‹¤.\n\nì•„ë˜ ë²„íŠ¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”:", reply_markup=keyboard)

@dp.callback_query(F.data == "filter_date")
async def callback_filter_date(callback: CallbackQuery, state: FSMContext):
    try:
        await callback.message.answer("MM/DD í˜•ì‹ìœ¼ë¡œ ë‚ ì§œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: 01/31)")
    except Exception:
        pass
    await state.set_state(FilterState.waiting_for_date)
    try:
        await callback.answer()
    except Exception:
        pass

@dp.callback_query(F.data == "all_notices")
async def callback_all_notices(callback: CallbackQuery, state: FSMContext):
    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text=category, callback_data=f"category_{code}")]
         for category, code in CATEGORY_CODES.items()
    ])
    try:
        await callback.message.answer("ì›í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:", reply_markup=keyboard)
    except Exception:
        pass
    await state.set_state(FilterState.selecting_category)
    try:
        await callback.answer()
    except Exception:
        pass

@dp.callback_query(F.data.startswith("category_"))
async def callback_category_selection(callback: CallbackQuery, state: FSMContext):
    category_code = callback.data.split("_")[1]
    notices = await get_school_notices(category_code)
    if not notices:
        try:
            await callback.message.answer("í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception:
            pass
    else:
        for notice in notices[:7]:
            await send_notification(notice)
    await state.clear()
    try:
        await callback.answer()
    except Exception:
        pass

@dp.message(F.text)
async def process_date_input(message: types.Message, state: FSMContext):
    current_state = await state.get_state()
    logging.info(f"Current FSM state raw: {current_state}")
    if current_state != FilterState.waiting_for_date.state:
        logging.warning("Received date input, but state is incorrect.")
        return
    input_text = message.text.strip()
    logging.info(f"Received date input: {input_text}")
    current_year = datetime.now().year
    full_date_str = f"{current_year}-{input_text.replace('/', '-')}"
    logging.info(f"Converted full date string: {full_date_str}")
    filter_date = parse_date(full_date_str)
    if filter_date is None:
        await message.answer("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. MM/DD í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        return
    notices = [n for n in await get_school_notices() if parse_date(n[3]) == filter_date]
    if not notices:
        logging.info(f"No notices found for {full_date_str}")
        await message.answer(f"ğŸ“¢ {input_text}ì˜ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        await message.answer(f"ğŸ“¢ {input_text}ì˜ ê³µì§€ì‚¬í•­ì…ë‹ˆë‹¤.", reply_markup=ReplyKeyboardRemove())
        for notice in notices:
            await send_notification(notice)
    logging.info("Clearing FSM state.")
    await state.clear()

async def run_bot():
    try:
        logging.info("ğŸš€ Starting bot polling for 10 minutes...")
        polling_task = asyncio.create_task(dp.start_polling(bot))
        await asyncio.sleep(600)
        logging.info("ğŸ›‘ Stopping bot polling after 10 minutes...")
        polling_task.cancel()
        await dp.stop_polling()
    except Exception as e:
        logging.error(f"âŒ Bot error: {e}")
    finally:
        await bot.session.close()
        logging.info("âœ… Bot session closed.")

if __name__ == '__main__':
    if sys.platform.startswith("win"):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    try:
        asyncio.run(run_bot())
    except RuntimeError as e:
        logging.error(f"âŒ asyncio ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

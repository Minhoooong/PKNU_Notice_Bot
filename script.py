import traceback
import logging
import asyncio
import sys
import aiohttp
from bs4 import BeautifulSoup
from collections import OrderedDict
from aiogram import Bot, Dispatcher, types, F
from aiogram.client.bot import DefaultBotProperties
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton, ReplyKeyboardRemove, CallbackQuery
from aiogram.filters import Command
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.context import FSMContext
import json
import os
import subprocess
import html
from datetime import datetime
import urllib.parse
import kss
import networkx as nx
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
openai.api_key = os.environ.get("OPENAI_API_KEY")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logfile.log"),
        logging.StreamHandler()
    ]
)

# --- ìƒìˆ˜ ë° í™˜ê²½ ë³€ìˆ˜ ---
URL = 'https://www.pknu.ac.kr/main/163'
BASE_URL = 'https://www.pknu.ac.kr'
CATEGORY_CODES = {
    "ì „ì²´": "",
    "ê³µì§€ì‚¬í•­": "10001",
    "ë¹„êµê³¼ ì•ˆë‚´": "10002",
    "í•™ì‚¬ ì•ˆë‚´": "10003",
    "ë“±ë¡/ì¥í•™": "10004",
    "ì´ˆë¹™/ì±„ìš©": "10007"
}
TOKEN = os.environ.get('TELEGRAM_TOKEN')
CHAT_ID = os.environ.get('CHAT_ID')

# --- ë´‡ ë° Dispatcher ì´ˆê¸°í™” ---
bot = Bot(token=TOKEN, default=DefaultBotProperties(parse_mode="HTML"))
dp = Dispatcher(bot=bot)

# --- FSM ìƒíƒœ ì •ì˜ ---
class FilterState(StatesGroup):
    waiting_for_date = State()
    selecting_category = State()

CACHE_FILE = "announcements_seen.json"

def truncate_text(text, max_length=3000):
    """
    ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ ê²½ìš° ì•ë¶€ë¶„ê³¼ ë’·ë¶€ë¶„ì„ ìœ ì§€í•˜ê³  ì¤‘ê°„ì„ ìƒëµí•˜ì—¬ ì••ì¶•.
    """
    if len(text) <= max_length:
        return text  # ê¸¸ì´ê°€ ì ë‹¹í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    half = max_length // 2
    return text[:half] + " ... (ì¤‘ëµ) ... " + text[-half:]  # ì•/ë’¤ ìœ ì§€, ì¤‘ê°„ ìƒëµ


def load_cache():
    """ ìºì‹œ íŒŒì¼ì—ì„œ ê¸°ì¡´ ê³µì§€ì‚¬í•­ ë¡œë“œ """
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_cache(data):
    """ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì„ ìºì‹œì— ì €ì¥ """
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def is_new_announcement(title, href):
    """ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì¸ì§€ í™•ì¸ """
    cache = load_cache()
    key = f"{title}::{href}"
    if key in cache:
        return False  # ì´ë¯¸ ì €ì¥ëœ ê³µì§€ì‚¬í•­ì´ë©´ False ë°˜í™˜
    cache[key] = True
    save_cache(cache)
    return True  # ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ë©´ True ë°˜í™˜

# --- ë‚ ì§œ íŒŒì‹± í•¨ìˆ˜ ---
def parse_date(date_str):
    try:
        return datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError as ve:
        logging.error(f"Date parsing error for {date_str}: {ve}")
        return None

async def fetch_url(url):
    """ ë¹„ë™ê¸° HTTP ìš”ì²­ í•¨ìˆ˜ """
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as response:
                if response.status != 200:
                    logging.error(f"âŒ HTTP ìš”ì²­ ì‹¤íŒ¨ ({response.status}): {url}")
                    return None
                return await response.text()
    except Exception as e:
        logging.error(f"âŒ URL ìš”ì²­ ì˜¤ë¥˜: {url}, {e}")
        logging.error(traceback.format_exc())  # âœ… traceback ì¶”ê°€
        return None

# --- ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ---
async def get_school_notices(category=""):
    try:
        category_url = f"{URL}?cd={category}" if category else URL
        html_content = await fetch_url(category_url)

        # âœ… URL ì‘ë‹µì´ Noneì´ë©´ ê³µì§€ì‚¬í•­ì„ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
        if html_content is None:
            logging.error(f"âŒ ê³µì§€ì‚¬í•­ í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {category_url}")
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
        notices = []

        for tr in soup.find_all("tr"):
            title_td = tr.find("td", class_="bdlTitle")
            user_td = tr.find("td", class_="bdlUser")
            date_td = tr.find("td", class_="bdlDate")
            
            if title_td and title_td.find("a") and user_td and date_td:
                a_tag = title_td.find("a")
                title = a_tag.get_text(strip=True)
                href = a_tag.get("href")

                # âœ… URLì´ ìƒëŒ€ ê²½ë¡œì¼ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if href.startswith("/"):
                    href = BASE_URL + href
                elif href.startswith("?"):
                    href = BASE_URL + "/main/163" + href
                elif not href.startswith("http"):
                    href = BASE_URL + "/" + href

                department = user_td.get_text(strip=True)
                date = date_td.get_text(strip=True)
                notices.append((title, href, department, date))

        notices.sort(key=lambda x: parse_date(x[3]) or datetime.min, reverse=True)
        return notices

    except Exception as e:
        logging.exception("âŒ Error in get_school_notices")
        return []

# --- TextRank ê¸°ë°˜ ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ ---
def text_rank_key_sentences(text, top_n=5):
    text = truncate_text(text, max_length=3000)  # âœ… ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¼ì„œ ìš”ì•½
    
    sentences = kss.split_sentences(text, backend="auto")
    
    if len(sentences) < 2:  
        logging.warning("âš ï¸ ë¬¸ì¥ ê°œìˆ˜ê°€ ë„ˆë¬´ ì ì–´ TextRank ì‹¤í–‰ ë¶ˆê°€.")
        return sentences  # âœ… ë¬¸ì¥ì´ 1ê°œ ì´í•˜ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    vectorizer = TfidfVectorizer(max_features=500)  # âœ… ë‹¨ì–´ ê°œìˆ˜ ì¦ê°€ (300 â†’ 500)
    try:
        sentence_vectors = vectorizer.fit_transform(sentences[:top_n*2]).toarray()
    except ValueError as e:
        logging.error(f"âŒ TF-IDF ë³€í™˜ ì˜¤ë¥˜: {e}")
        return sentences[:top_n]  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œ ì¼ë¶€ ë¬¸ì¥ ë°˜í™˜

    if sentence_vectors.shape[0] < 2:  
        logging.warning("âš ï¸ ë¬¸ì¥ì´ ë¶€ì¡±í•˜ì—¬ TextRank ì‹¤í–‰ ë¶ˆê°€.")
        return sentences  # âœ… ì›ë³¸ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜

    similarity_matrix = cosine_similarity(sentence_vectors, sentence_vectors)
    nx_graph = nx.from_numpy_array(similarity_matrix)

    try:
        scores = nx.pagerank(nx_graph)
    except Exception as e:
        logging.error(f"âŒ PageRank ì˜¤ë¥˜: {e}")
        return sentences[:top_n]  # âœ… ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¼ë¶€ ë¬¸ì¥ ë°˜í™˜

    ranked_sentences = sorted(
        ((scores.get(i, 0), s) for i, s in enumerate(sentences) if i in scores), 
        reverse=True
    )

    return [s for _, s in ranked_sentences[:top_n]] if ranked_sentences else sentences[:top_n]

def clean_and_format_text(text):
    """
    - ì¤‘ë³µ ë‹¨ì–´ ë° ë°˜ë³µëœ í‘œí˜„ ì œê±°
    - ë¬¸ì¥ ë§ˆì¹¨í‘œ ì¶”ê°€
    - ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
    """
    if not text.strip():
        return text  # ë¹ˆ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    # 1ï¸âƒ£ ì¤‘ë³µ ë‹¨ì–´ ë° ë°˜ë³µëœ í‘œí˜„ ì œê±° (ì •ê·œì‹ ëŒ€ì‹  OrderedDict í™œìš©)
    words = text.split()
    cleaned_words = list(OrderedDict.fromkeys(words))  # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
    text = " ".join(cleaned_words)

    # 2ï¸âƒ£ ë¬¸ì¥ ë§ˆì¹¨í‘œ ë³´ì •
    sentences = kss.split_sentences(text, backend="auto")  # ë¬¸ì¥ ë¶„ë¦¬
    cleaned_sentences = []
    for sentence in sentences:
        if not sentence.endswith(('.', '!', '?', '"', "'")):
            sentence += "."  # ë¬¸ì¥ ëì´ ì´ìƒí•˜ë©´ ë§ˆì¹¨í‘œ ì¶”ê°€
        cleaned_sentences.append(sentence)

    # 3ï¸âƒ£ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì ìš© (ê°€ë…ì„± í–¥ìƒ)
    formatted_text = "\n".join(cleaned_sentences).strip()  # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°

    return formatted_text

def extract_key_sentences(text, top_n=5):
    """
    ì¤‘ìš” ë¬¸ì¥ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
    TextRank ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ìƒìœ„ Nê°œì˜ ë¬¸ì¥ì„ ì„ íƒ.
    """
    sentences = kss.split_sentences(text, backend="auto")
    
    # ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” ë‹¨ì–´ë¥¼ ì„ ë³„
    word_count = Counter(" ".join(sentences).split())
    important_words = [word for word, count in word_count.most_common(20)]  # ìƒìœ„ 20ê°œ ë‹¨ì–´ ì„ íƒ
    
    # ì¤‘ìš” ë‹¨ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ í•„í„°ë§
    key_sentences = []
    for sentence in sentences:
        if any(word in sentence for word in important_words):
            key_sentences.append(sentence)

    return key_sentences[:top_n]  # ìƒìœ„ Nê°œ ë¬¸ì¥ ì„ íƒ

# --- í…ìŠ¤íŠ¸ ìš”ì•½ ---
def summarize_text(text):
    """
    GPT-4o Minië¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìš”ì•½.
    """
    if text is None or not text.strip():
        return "ìš”ì•½í•  ìˆ˜ ì—†ëŠ” ê³µì§€ì…ë‹ˆë‹¤."

    # TextRank ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•´ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ê¸´ í…ìŠ¤íŠ¸ ëŒ€ë¹„ ë¹„ìš© ì ˆê°)
    key_sentences = text_rank_key_sentences(text, top_n=10)  # âœ… ìš”ì•½ í’ˆì§ˆ í–¥ìƒ

    if not key_sentences:
        return "ìš”ì•½í•  ìˆ˜ ì—†ëŠ” ê³µì§€ì…ë‹ˆë‹¤."

    combined_text = " ".join(key_sentences)  # í•µì‹¬ ë¬¸ì¥ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
    prompt = f"ë‹¤ìŒ ê³µì§€ì‚¬í•­ì„ 3~5 ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜:\n\n{combined_text}\n\nìš”ì•½:"

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",  # âœ… GPT-4o Mini ëª¨ë¸ ì‚¬ìš©
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,  # ì‘ë‹µì˜ ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
            max_tokens=300  # âœ… í† í° ì œí•œ í•´ì œ (ë” ê¸¸ê²Œ ìš”ì•½ ê°€ëŠ¥)
        )
        summary = response["choices"][0]["message"]["content"].strip()
        return summary

    except Exception as e:
        logging.error(f"âŒ OpenAI API ìš”ì•½ ì˜¤ë¥˜: {e}")
        return "ìš”ì•½í•  ìˆ˜ ì—†ëŠ” ê³µì§€ì…ë‹ˆë‹¤."
        
# --- ì½˜í…ì¸  ì¶”ì¶œ: bdvTxt_wrap ì˜ì—­ ë‚´ í…ìŠ¤íŠ¸ì™€ /upload/ ì´ë¯¸ì§€ í¬ë¡¤ë§ ---
async def extract_content(url):
    try:
        html_content = await fetch_url(url)
        if html_content is None or len(html_content.strip()) == 0:
            logging.error(f"âŒ Failed to fetch content: {url}")
            return "í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

        soup = BeautifulSoup(html_content, 'html.parser')
        container = soup.find("div", class_="bdvTxt_wrap")
        if not container:
            container = soup

        paragraphs = container.find_all('p')
        if not paragraphs:
            logging.error(f"âŒ No text content found in {url}")
            return "ë³¸ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.", []

        raw_text = ' '.join([para.get_text(separator=" ", strip=True) for para in paragraphs])

        if raw_text.strip():
            summary_text = summarize_text(raw_text)  # âœ… GPT-4o Mini ì‚¬ìš©
        else:
            summary_text = "ë³¸ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

        images = [urllib.parse.urljoin(url, img['src']) for img in container.find_all('img') if "/upload/" in img['src']]
        return summary_text, images

    except Exception as e:
        logging.error(f"âŒ Exception in extract_content for URL {url}: {e}")
        return "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", []

async def is_valid_url(url):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.head(url, timeout=10) as response:
                return response.status == 200
    except Exception as e:
        logging.error(f"âŒ Invalid image URL: {url}, error: {e}")
    return False

# --- JSON íŒŒì¼ ì²˜ë¦¬ (ê³µì§€ì‚¬í•­ ì¤‘ë³µ ì²´í¬) ---
def load_seen_announcements():
    try:
        with open("announcements_seen.json", "r", encoding="utf-8") as f:
            seen_data = json.load(f)
            return {(item[0], item[1]) if len(item) == 2 else tuple(item) for item in seen_data}
    except (FileNotFoundError, json.JSONDecodeError):
        return set()

def save_seen_announcements(seen):
    try:
        with open("announcements_seen.json", "w", encoding="utf-8") as f:
            json.dump([list(item) for item in seen], f, ensure_ascii=False, indent=4)
        push_changes()
    except Exception as e:
        logging.error(f"âŒ Failed to save announcements_seen.json and push to GitHub: {e}")

def push_changes():
    try:
        pat = os.environ.get("MY_PAT")
        if not pat:
            logging.error("âŒ GitHub PATê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Pushë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
            return
        os.environ["GIT_ASKPASS"] = "echo"
        os.environ["GIT_PASSWORD"] = pat
        subprocess.run(["git", "config", "--global", "credential.helper", "store"], check=True)
        subprocess.run(["git", "add", "announcements_seen.json"], check=True)
        subprocess.run(["git", "commit", "-m", "Update announcements_seen.json"], check=True)
        subprocess.run(["git", "push", "origin", "main"], check=True)
        logging.info("âœ… Successfully pushed changes to GitHub.")
    except subprocess.CalledProcessError as e:
        logging.error(f"âŒ ERROR: Failed to push changes to GitHub: {e}")

async def check_for_new_notices():
    logging.info("Checking for new notices...")
    seen_announcements = load_seen_announcements()
    logging.info(f"Loaded seen announcements: {seen_announcements}")
    current_notices = await get_school_notices()
    logging.info(f"Fetched current notices: {current_notices}")
    seen_titles_urls = {(title, url) for title, url, *_ in seen_announcements}
    new_notices = [
        (title, href, department, date) for title, href, department, date in current_notices
        if (title, href) not in seen_titles_urls
    ]
    logging.info(f"DEBUG: New notices detected: {new_notices}")
    if new_notices:
        for notice in new_notices:
            await send_notification(notice)
        seen_announcements.update(new_notices)
        save_seen_announcements(seen_announcements)
        logging.info(f"DEBUG: Updated seen announcements (after update): {seen_announcements}")
    else:
        logging.info("âœ… ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

@dp.message(Command("checknotices"))
async def manual_check_notices(message: types.Message):
    new_notices = await check_for_new_notices()
    if new_notices:
        await message.answer(f"ğŸ“¢ {len(new_notices)}ê°œì˜ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤!")
    else:
        await message.answer("âœ… ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

async def send_notification(notice):
    title, href, department, date = notice
    summary_text, image_urls = await extract_content(href)

    # âœ… summary_textê°€ Noneì´ë©´ ê¸°ë³¸ ë©”ì‹œì§€ ì‚¬ìš©
    if summary_text is None:
        summary_text = ""

    message_text = f"[ë¶€ê²½ëŒ€ <b>{html.escape(department)}</b> ê³µì§€ì‚¬í•­ ì—…ë°ì´íŠ¸]\n\n"
    message_text += f"<b>{html.escape(title)}</b>\n\n{html.escape(date)}\n\n"
    message_text += f"{html.escape(summary_text)}"

    if image_urls:
        message_text += "\n\n[ì²¨ë¶€ ì´ë¯¸ì§€]\n" + "\n".join(image_urls)

    keyboard = InlineKeyboardMarkup(inline_keyboard=[[InlineKeyboardButton(text="ìì„¸íˆ ë³´ê¸°", url=href)]])
    await bot.send_message(chat_id=CHAT_ID, text=message_text, reply_markup=keyboard)

@dp.message(Command("start"))
async def start_command(message: types.Message):
    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="ğŸ“…ë‚ ì§œ ì…ë ¥", callback_data="filter_date"),
         InlineKeyboardButton(text="ğŸ“¢ì „ì²´ ê³µì§€ì‚¬í•­", callback_data="all_notices")]
    ])
    await message.answer("ì•ˆë…•í•˜ì„¸ìš”! ê³µì§€ì‚¬í•­ ë´‡ì…ë‹ˆë‹¤.\n\nì•„ë˜ ë²„íŠ¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”:", reply_markup=keyboard)

@dp.callback_query(F.data == "filter_date")
async def callback_filter_date(callback: CallbackQuery, state: FSMContext):
    try:
        await callback.message.answer("MM/DD í˜•ì‹ìœ¼ë¡œ ë‚ ì§œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: 01/31)")
    except Exception:
        pass
    await state.set_state(FilterState.waiting_for_date)
    try:
        await callback.answer()
    except Exception:
        pass

@dp.callback_query(F.data == "all_notices")
async def callback_all_notices(callback: CallbackQuery, state: FSMContext):
    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text=category, callback_data=f"category_{code}")]
         for category, code in CATEGORY_CODES.items()
    ])
    try:
        await callback.message.answer("ì›í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:", reply_markup=keyboard)
    except Exception:
        pass
    await state.set_state(FilterState.selecting_category)
    try:
        await callback.answer()
    except Exception:
        pass

@dp.callback_query(F.data.startswith("category_"))
async def callback_category_selection(callback: CallbackQuery, state: FSMContext):
    category_code = callback.data.split("_")[1]
    notices = await get_school_notices(category_code)
    if not notices:
        try:
            await callback.message.answer("í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception:
            pass
    else:
        for notice in notices[:7]:
            await send_notification(notice)
    await state.clear()
    try:
        await callback.answer()
    except Exception:
        pass

@dp.message(F.text)
async def process_date_input(message: types.Message, state: FSMContext):
    current_state = await state.get_state()
    logging.info(f"Current FSM state raw: {current_state}")
    if current_state != FilterState.waiting_for_date.state:
        logging.warning("Received date input, but state is incorrect.")
        return
    input_text = message.text.strip()
    logging.info(f"Received date input: {input_text}")
    current_year = datetime.now().year
    full_date_str = f"{current_year}-{input_text.replace('/', '-')}"
    logging.info(f"Converted full date string: {full_date_str}")
    filter_date = parse_date(full_date_str)
    if filter_date is None:
        await message.answer("ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. MM/DD í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        return
    notices = [n for n in await get_school_notices() if parse_date(n[3]) == filter_date]
    if not notices:
        logging.info(f"No notices found for {full_date_str}")
        await message.answer(f"ğŸ“¢ {input_text}ì˜ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        await message.answer(f"ğŸ“¢ {input_text}ì˜ ê³µì§€ì‚¬í•­ì…ë‹ˆë‹¤.", reply_markup=ReplyKeyboardRemove())
        for notice in notices:
            await send_notification(notice)
    logging.info("Clearing FSM state.")
    await state.clear()

async def run_bot():
    try:
        logging.info("ğŸš€ Starting bot polling for 10 minutes...")
        polling_task = asyncio.create_task(dp.start_polling(bot))
        await asyncio.sleep(600)
        logging.info("ğŸ›‘ Stopping bot polling after 10 minutes...")
        polling_task.cancel()
        await dp.stop_polling()
    except Exception as e:
        logging.error(f"âŒ Bot error: {e}")
    finally:
        await bot.session.close()
        logging.info("âœ… Bot session closed.")

if __name__ == '__main__':
    if sys.platform.startswith("win"):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    try:
        asyncio.run(run_bot())
    except RuntimeError as e:
        logging.error(f"âŒ asyncio ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
